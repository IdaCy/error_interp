# Error Mech Analysis

This is a research project by IMI (Imperial Mechanistic Interpretability) exploring how Large Language Models (LLMs) process and respond to *semantically equivalent* instructions or questions that differ in style, tone, or structure. 

The repository focuses on:
- **Typo Prompt Generation** (e.g. polite vs. urgent rephrasings)
- **Mechanistic Interpretability** (internal attention and neuron activation differences)
- **Evaluation Metrics** (using open-source reward models or GPT-4 to score responses)
- **Analysis** of .pt files containing hidden states, attentions, and outputs

This work aims to shed light on how LLMs encode typo-introduced queries and to measure the reliability of their output under varied phrasing.

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [Installation](#installation)
3. [Usage](#usage)
   - [Inference Pipeline](#inference-pipeline)
   - [Scoring and Evaluation](#scoring-and-evaluation)
4. [Data](#data)
5. [Directory Structure](#directory-structure)
6. [Contributing](#contributing)
7. [License](#license)

---

## Project Overview

Modern LLMs can perform differently even if two prompts share the same meaning but differ in wording. This repository investigates that phenomenon by:

- **Generating** typo prompts (e.g. "main” word changed, “swap,” etc.).
- **Running inference** on an LLM to capture hidden representations (attention weights, embeddings).
- **Scoring** the generated outputs using a reward model (like [OpenAssistant’s DeBERTa-based RM](https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2)) to gauge performance differences among typo strings.
- **Analyzing** internal activations to identify which layers/heads are sensitive to style or wording changes.

---

## Installation

1. Clone the repository:
   ```
   git clone https://github.com/IdaCy/error_interp.git
   cd error_interp
   ```

2. Create (and activate) a virtual environment (optional, but recommended):
   ```
   python -m venv venv
   source venv/bin/activate  # on Linux/Mac
   # or .\venv\Scripts\activate on Windows
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

   The main packages are:
   - [transformers](https://github.com/huggingface/transformers)
   - [accelerate](https://github.com/huggingface/accelerate)
   - [torch](https://pytorch.org/)
   - [numpy / pandas] (for data handling, optional)
   - [tqdm] (for progress bars, optional)

---

## Usage

### Inference Pipeline

**Scripts**:
- [`inference.py`](./inference.py) or the provided code snippet that:
  1. Loads a pre-trained LLM (e.g. `google/gemma-2-9b-it`).
  2. Reads a JSON of user prompts (or typo versions).
  3. Runs inference in batches, capturing hidden states, attentions, top-k logits, final predictions, etc.
  4. Saves them to `.pt` files in an output directory.

**Running**:
```
python inference.py \
  --model_name google/gemma-2-9b-it \
  --prompt_file data/polite_prompts.json \
  --output_dir outputs/polite_run
```

You can do this for each typo style (e.g. “main,” word changed “swap,” in charecters etc.) to produce multiple `.pt` files containing your model’s outputs.

### Scoring and Evaluation

**Scripts**:
- [`score_scripts.py`](./score_scripts.py): A modular set of functions that:
  1. Loads all `.pt` files from a directory.
  2. Initializes the [OpenAssistant/deberta-v3-based reward model](https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2).
  3. Scores each final prediction in mini-batches.
  4. Writes out CSV with raw scores and a JSON/TXT with aggregated min, max, average, etc.

**Example** in a Colab cell:
```
!pip install transformers accelerate

from score_scripts import (
    load_reward_model,
    score_predictions_and_save
)

# Directory where .pt files are stored (generated by inference.py)
pt_dir = "outputs/polite_run"

# CSV output and aggregate paths
raw_csv_path = "results/polite_scores.csv"
agg_base_path = "results/polite_aggregates"

rm_name = "OpenAssistant/reward-model-deberta-v3-large-v2"
rm_pipeline = load_reward_model(rm_name, device=0)  # or "cuda:0"

scored_list = score_predictions_and_save(
    pt_dir=pt_dir,
    rm_pipeline=rm_pipeline,
    raw_csv_path=raw_csv_path,
    agg_base_path=agg_base_path
)

if len(scored_list) > 0:
    avg_score = sum(x["reward_score"] for x in scored_list) / len(scored_list)
    print(f"Average score for 'polite' prompts: {avg_score:.3f}")
else:
    print("No items scored!")
```

---

## Data

You’ll need to provide your own typo-introduced prompt data. Some typical workflows:
- Generate typo version with a strong LLM (like GPT-4).
- Use *existing* datasets (Quora, PAWS, etc.) and adapt them into an instruction style.

**Example**:
```
[
  { "normal": "Explain the concept of backpropagation in neural networks." },
  { "other": "Describe how backpropagation works when training a neural net." },
  ...
]
```


---

## Directory Structure

Structure initially:

```
error_interp/
├── 0_data/
│   ├── maths.json         <- put your JSON here
│   └── instructions.json
├── 1_utils/
│   ├── hpc/
│   └── run_scripts/       <- always useful scripts
│       ├── load_json.py
│       ├── load_model.py
│       ├── logger.py
│       └── read_predictions.py
├── 2_inference/
│   ├── hpc/
│   └── run_scripts/
│       └── attn_act_logi.py
├── 3_maths_eval/
│   ├── hpc/
│   └── run_scripts/
│       ├── parse_predictions.py
│       └── evaluate_predictions.py
├── 4_semantics_eval/
│   ├── hpc/
│   └── run_scripts/
├── 5_attention_fractions/
│   ├── hpc/
│   └── run_scripts/
│       └── dirs_eval.py
├── requirements.txt
└── README.md
```

- `data/` holds JSON prompt sets.
- `outputs/` is where `.pt` files get saved after the LLM runs.
- `results/` holds any CSV or aggregate stats from the scoring scripts.

---

## Contributing

1. **Fork** the repository.
2. **Create a branch** for your feature/patch.
3. **Submit a Pull Request** describing changes and test results.
4. For major contributions or architectural changes, please open an [Issue](https://github.com/IdaCy/error_interp/issues) first.

We welcome feedback, bug reports, and discussion!

---

## License

This project is licensed under the [MIT License](./LICENSE).
